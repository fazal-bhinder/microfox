{
  "url": "https://docs.aws.amazon.com/rekognition/latest/dg/faces-detect-images.html",
  "content": "English\nPreferences \nContact Us\nFeedback\nGet started\nService guides\nDeveloper tools\nAI resources\nDocumentation\nAmazon Rekognition\nDeveloper Guide\nDocumentation\nAmazon Rekognition\nDeveloper Guide\nDetecting faces in an image\n PDF\n RSS\nFocus mode\n\nAmazon Rekognition Image provides the DetectFaces operation that looks for key facial features such as eyes, nose, and mouth to detect faces in an input image. Amazon Rekognition Image detects the 100 largest faces in an image.\n\nYou can provide the input image as an image byte array (base64-encoded image bytes), or specify an Amazon S3 object. In this procedure, you upload an image (JPEG or PNG) to your S3 bucket and specify the object key name.\n\nTo detect faces in an image\n\nIf you haven't already:\n\nCreate or update a user with AmazonRekognitionFullAccess and AmazonS3ReadOnlyAccess permissions. For more information, see Step 1: Set up an AWS account and create a User.\n\nInstall and configure the AWS CLI and the AWS SDKs. For more information, see Step 2: Set up the AWS CLI and AWS SDKs.\n\nUpload an image (that contains one or more faces) to your S3 bucket.\n\nFor instructions, see Uploading Objects into Amazon S3 in the Amazon Simple Storage Service User Guide.\n\nUse the following examples to call DetectFaces.\n\nJava\nJava V2\nAWS CLI\nPython\n.NET\nRuby\nNode.js\n\nThis example displays the estimated age range for detected faces, and lists the JSON for all detected facial attributes. Change the value of photo to the image file name. Change the value of amzn-s3-demo-bucket to the Amazon S3 bucket where the image is stored.\n\n//Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n//PDX-License-Identifier: MIT-0 (For details, see https://github.com/awsdocs/amazon-rekognition-developer-guide/blob/master/LICENSE-SAMPLECODE.)\n\npackage aws.example.rekognition.image;\n\nimport com.amazonaws.services.rekognition.AmazonRekognition;\nimport com.amazonaws.services.rekognition.AmazonRekognitionClientBuilder;\nimport com.amazonaws.services.rekognition.model.AmazonRekognitionException;\nimport com.amazonaws.services.rekognition.model.Image;\nimport com.amazonaws.services.rekognition.model.S3Object;\nimport com.amazonaws.services.rekognition.model.AgeRange;\nimport com.amazonaws.services.rekognition.model.Attribute;\nimport com.amazonaws.services.rekognition.model.DetectFacesRequest;\nimport com.amazonaws.services.rekognition.model.DetectFacesResult;\nimport com.amazonaws.services.rekognition.model.FaceDetail;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport java.util.List;\n\n\npublic class DetectFaces {\n   \n   \n   public static void main(String[] args) throws Exception {\n\n      String photo = \"input.jpg\";\n      String bucket = \"bucket\";\n\n      AmazonRekognition rekognitionClient = AmazonRekognitionClientBuilder.defaultClient();\n\n\n      DetectFacesRequest request = new DetectFacesRequest()\n         .withImage(new Image()\n            .withS3Object(new S3Object()\n               .withName(photo)\n               .withBucket(bucket)))\n         .withAttributes(Attribute.ALL);\n      // Replace Attribute.ALL with Attribute.DEFAULT to get default values.\n\n      try {\n         DetectFacesResult result = rekognitionClient.detectFaces(request);\n         List < FaceDetail > faceDetails = result.getFaceDetails();\n\n         for (FaceDetail face: faceDetails) {\n            if (request.getAttributes().contains(\"ALL\")) {\n               AgeRange ageRange = face.getAgeRange();\n               System.out.println(\"The detected face is estimated to be between \"\n                  + ageRange.getLow().toString() + \" and \" + ageRange.getHigh().toString()\n                  + \" years old.\");\n               System.out.println(\"Here's the complete set of attributes:\");\n            } else { // non-default attributes have null values.\n               System.out.println(\"Here's the default set of attributes:\");\n            }\n\n            ObjectMapper objectMapper = new ObjectMapper();\n            System.out.println(objectMapper.writerWithDefaultPrettyPrinter().writeValueAsString(face));\n         }\n\n      } catch (AmazonRekognitionException e) {\n         e.printStackTrace();\n      }\n\n   }\n\n}\n\n\nDetectFaces operation request\n\nThe input to DetectFaces is an image. In this example, the image is loaded from an Amazon S3 bucket. The Attributes parameter specifies that all facial attributes should be returned. For more information, see Working with images.\n\n{\n    \"Image\": {\n        \"S3Object\": {\n            \"Bucket\": \"amzn-s3-demo-bucket\",\n            \"Name\": \"input.jpg\"\n        }\n    },\n    \"Attributes\": [\n        \"ALL\"\n    ]\n}\nDetectFaces operation response\n\nDetectFaces returns the following information for each detected face:\n\nBounding box – The coordinates of the bounding box that surrounds the face.\n\nConfidence – The level of confidence that the bounding box contains a face.\n\nFacial landmarks – An array of facial landmarks. For each landmark (such as the left eye, right eye, and mouth), the response provides the x and y coordinates.\n\nFacial attributes – A set of facial attributes, such as whether the face is occluded, returned as a FaceDetail object. The set includes: AgeRange, Beard, Emotions, EyeDirection, Eyeglasses, EyesOpen, FaceOccluded, Gender, MouthOpen, Mustache, Smile, and Sunglasses. For each such attribute, the response provides a value. The value can be of different types, such as a Boolean type (whether a person is wearing sunglasses), a string (whether the person is male or female), or an angular degree value (for pitch/yaw of eye gaze directions). In addition, for most attributes, the response also provides a confidence in the detected value for the attribute. Note that while FaceOccluded and EyeDirection attributes are supported when using DetectFaces, they aren't supported when analyzing videos with StartFaceDetection and GetFaceDetection.\n\nQuality – Describes the brightness and the sharpness of the face. For information about ensuring the best possible face detection, see Recommendations for facial comparison input images.\n\nPose – Describes the rotation of the face inside the image.\n\nThe request can depict an array of facial attributes you want to be returned. A DEFAULT subset of facial attributes - BoundingBox, Confidence, Pose, Quality, and Landmarks - will always be returned. You can request the return of specific facial attributes (in addition to the default list) - by using [\"DEFAULT\", \"FACE_OCCLUDED\", \"EYE_DIRECTION\"] or just one attribute, like [\"FACE_OCCLUDED\"]. You can request for all facial attributes by using [\"ALL\"]. Requesting more attributes may increase response time.\n\nThe following is an example response of a DetectFaces API call:\n\n{\n  \"FaceDetails\": [\n    {\n      \"BoundingBox\": {\n        \"Width\": 0.7919622659683228,\n        \"Height\": 0.7510867118835449,\n        \"Left\": 0.08881539851427078,\n        \"Top\": 0.151064932346344\n      },\n      \"AgeRange\": {\n        \"Low\": 18,\n        \"High\": 26\n      },\n      \"Smile\": {\n        \"Value\": false,\n        \"Confidence\": 89.77348327636719\n      },\n      \"Eyeglasses\": {\n        \"Value\": true,\n        \"Confidence\": 99.99996948242188\n      },\n      \"Sunglasses\": {\n        \"Value\": true,\n        \"Confidence\": 93.65237426757812\n      },\n      \"Gender\": {\n        \"Value\": \"Female\",\n        \"Confidence\": 99.85968780517578\n      },\n      \"Beard\": {\n        \"Value\": false,\n        \"Confidence\": 77.52591705322266\n      },\n      \"Mustache\": {\n        \"Value\": false,\n        \"Confidence\": 94.48904418945312\n      },\n      \"EyesOpen\": {\n        \"Value\": true,\n        \"Confidence\": 98.57169342041016\n      },\n      \"MouthOpen\": {\n        \"Value\": false,\n        \"Confidence\": 74.33953094482422\n      },\n      \"Emotions\": [\n        {\n          \"Type\": \"SAD\",\n          \"Confidence\": 65.56403350830078\n        },\n        {\n          \"Type\": \"CONFUSED\",\n          \"Confidence\": 31.277774810791016\n        },\n        {\n          \"Type\": \"DISGUSTED\",\n          \"Confidence\": 15.553778648376465\n        },\n        {\n          \"Type\": \"ANGRY\",\n          \"Confidence\": 8.012762069702148\n        },\n        {\n          \"Type\": \"SURPRISED\",\n          \"Confidence\": 7.621500015258789\n        },\n        {\n          \"Type\": \"FEAR\",\n          \"Confidence\": 7.243380546569824\n        },\n        {\n          \"Type\": \"CALM\",\n          \"Confidence\": 5.8196024894714355\n        },\n        {\n          \"Type\": \"HAPPY\",\n          \"Confidence\": 2.2830512523651123\n        }\n      ],\n      \"Landmarks\": [\n        {\n          \"Type\": \"eyeLeft\",\n          \"X\": 0.30225440859794617,\n          \"Y\": 0.41018882393836975\n        },\n        {\n          \"Type\": \"eyeRight\",\n          \"X\": 0.6439348459243774,\n          \"Y\": 0.40341562032699585\n        },\n        {\n          \"Type\": \"mouthLeft\",\n          \"X\": 0.343580037355423,\n          \"Y\": 0.6951127648353577\n        },\n        {\n          \"Type\": \"mouthRight\",\n          \"X\": 0.6306480765342712,\n          \"Y\": 0.6898072361946106\n        },\n        {\n          \"Type\": \"nose\",\n          \"X\": 0.47164231538772583,\n          \"Y\": 0.5763645172119141\n        },\n        {\n          \"Type\": \"leftEyeBrowLeft\",\n          \"X\": 0.1732882857322693,\n          \"Y\": 0.34452149271965027\n        },\n        {\n          \"Type\": \"leftEyeBrowRight\",\n          \"X\": 0.3655243515968323,\n          \"Y\": 0.33231860399246216\n        },\n        {\n          \"Type\": \"leftEyeBrowUp\",\n          \"X\": 0.2671719491481781,\n          \"Y\": 0.31669262051582336\n        },\n        {\n          \"Type\": \"rightEyeBrowLeft\",\n          \"X\": 0.5613729953765869,\n          \"Y\": 0.32813435792922974\n        },\n        {\n          \"Type\": \"rightEyeBrowRight\",\n          \"X\": 0.7665090560913086,\n          \"Y\": 0.3318614959716797\n        },\n        {\n          \"Type\": \"rightEyeBrowUp\",\n          \"X\": 0.6612788438796997,\n          \"Y\": 0.3082450032234192\n        },\n        {\n          \"Type\": \"leftEyeLeft\",\n          \"X\": 0.2416982799768448,\n          \"Y\": 0.4085965156555176\n        },\n        {\n          \"Type\": \"leftEyeRight\",\n          \"X\": 0.36943578720092773,\n          \"Y\": 0.41230902075767517\n        },\n        {\n          \"Type\": \"leftEyeUp\",\n          \"X\": 0.29974061250686646,\n          \"Y\": 0.3971870541572571\n        },\n        {\n          \"Type\": \"leftEyeDown\",\n          \"X\": 0.30360740423202515,\n          \"Y\": 0.42347756028175354\n        },\n        {\n          \"Type\": \"rightEyeLeft\",\n          \"X\": 0.5755768418312073,\n          \"Y\": 0.4081145226955414\n        },\n        {\n          \"Type\": \"rightEyeRight\",\n          \"X\": 0.7050536870956421,\n          \"Y\": 0.39924031496047974\n        },\n        {\n          \"Type\": \"rightEyeUp\",\n          \"X\": 0.642906129360199,\n          \"Y\": 0.39026668667793274\n        },\n        {\n          \"Type\": \"rightEyeDown\",\n          \"X\": 0.6423097848892212,\n          \"Y\": 0.41669243574142456\n        },\n        {\n          \"Type\": \"noseLeft\",\n          \"X\": 0.4122826159000397,\n          \"Y\": 0.5987403392791748\n        },\n        {\n          \"Type\": \"noseRight\",\n          \"X\": 0.5394935011863708,\n          \"Y\": 0.5960900187492371\n        },\n        {\n          \"Type\": \"mouthUp\",\n          \"X\": 0.478581964969635,\n          \"Y\": 0.6660456657409668\n        },\n        {\n          \"Type\": \"mouthDown\",\n          \"X\": 0.483366996049881,\n          \"Y\": 0.7497162818908691\n        },\n        {\n          \"Type\": \"leftPupil\",\n          \"X\": 0.30225440859794617,\n          \"Y\": 0.41018882393836975\n        },\n        {\n          \"Type\": \"rightPupil\",\n          \"X\": 0.6439348459243774,\n          \"Y\": 0.40341562032699585\n        },\n        {\n          \"Type\": \"upperJawlineLeft\",\n          \"X\": 0.11031254380941391,\n          \"Y\": 0.3980775475502014\n        },\n        {\n          \"Type\": \"midJawlineLeft\",\n          \"X\": 0.19301874935626984,\n          \"Y\": 0.7034031748771667\n        },\n        {\n          \"Type\": \"chinBottom\",\n          \"X\": 0.4939905107021332,\n          \"Y\": 0.8877836465835571\n        },\n        {\n          \"Type\": \"midJawlineRight\",\n          \"X\": 0.7990140914916992,\n          \"Y\": 0.6899225115776062\n        },\n        {\n          \"Type\": \"upperJawlineRight\",\n          \"X\": 0.8548634648323059,\n          \"Y\": 0.38160091638565063\n        }\n      ],\n      \"Pose\": {\n        \"Roll\": -5.83309268951416,\n        \"Yaw\": -2.4244730472564697,\n        \"Pitch\": 2.6216139793395996\n      },\n      \"Quality\": {\n        \"Brightness\": 96.16363525390625,\n        \"Sharpness\": 95.51618957519531\n      },\n      \"Confidence\": 99.99872589111328,\n      \"FaceOccluded\": {\n        \"Value\": true,\n        \"Confidence\": 99.99726104736328\n      },\n      \"EyeDirection\": {\n        \"Yaw\": 16.299732,\n        \"Pitch\": -6.407457,\n        \"Confidence\": 99.968704\n      }\n    }\n  ],\n  \"ResponseMetadata\": {\n    \"RequestId\": \"8bf02607-70b7-4f20-be55-473fe1bba9a2\",\n    \"HTTPStatusCode\": 200,\n    \"HTTPHeaders\": {\n      \"x-amzn-requestid\": \"8bf02607-70b7-4f20-be55-473fe1bba9a2\",\n      \"content-type\": \"application/x-amz-json-1.1\",\n      \"content-length\": \"3409\",\n      \"date\": \"Wed, 26 Apr 2023 20:18:50 GMT\"\n    },\n    \"RetryAttempts\": 0\n  }\n}\n\nNote the following:\n\nThe Pose data describes the rotation of the face detected. You can use the combination of the BoundingBox and Pose data to draw the bounding box around faces that your application displays.\n\nThe Quality describes the brightness and the sharpness of the face. You might find this useful to compare faces across images and find the best face.\n\nThe preceding response shows all facial landmarks the service can detect, all facial attributes and emotions. To get all of these in the response, you must specify the attributes parameter with value ALL. By default, the DetectFaces API returns only the following five facial attributes: BoundingBox, Confidence, Pose, Quality and landmarks. The default landmarks returned are: eyeLeft, eyeRight, nose, mouthLeft, and mouthRight.\n\nOn this page\nDid this page help you?\nYes\nNo\nProvide feedback\nNext topic:Comparing faces in images\nPrevious topic:Guidelines on face attributes\nNeed help?\nTry AWS re:Post \nConnect with an AWS IQ expert \nPrivacySite termsCookie preferences\n© 2025, Amazon Web Services, Inc. or its affiliates. All rights reserved.",
  "updatedAt": "2025-05-05T20:32:00.090Z"
}